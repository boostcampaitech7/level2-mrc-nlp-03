{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki_data = json.load(f)\n",
    "\n",
    "documents = [v['text'] for v in wiki_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'retrieve_result.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "bm25_failures = df[df['bm25'] == '20+']\n",
    "\n",
    "failed_queries = bm25_failures['question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "tokenized_corpus = [doc.split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "\n",
    "for query in failed_queries:\n",
    "    # IDF 계산\n",
    "    tokens = vectorizer.transform([query])\n",
    "    idf_values = {word: vectorizer.idf_[vectorizer.vocabulary_[word]] \n",
    "                  for word in query.split() if word in vectorizer.vocabulary_}\n",
    "    \n",
    "    # BM25 상위 20개 문서 번호 추출\n",
    "    tokenized_query = query.split()  \n",
    "    doc_scores = bm25.get_scores(tokenized_query) \n",
    "    top_n_indices = doc_scores.argsort()[::-1][:20]  \n",
    "    top_n_doc_ids = top_n_indices.tolist()\n",
    "\n",
    "    # 데이터 저장\n",
    "    output_data.append({\n",
    "        'question': query,\n",
    "        'idf_values': idf_values,\n",
    "        'bm25_top_20': top_n_doc_ids\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 실패 데이터 분석 결과가 'bm25_failure_analysis.csv'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_csv('bm25_failure_analysis.csv', index=False)\n",
    "\n",
    "print(\"BM25 실패 데이터 분석 결과가 'bm25_failure_analysis.csv'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 한국어 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Konply - okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Kkma, Komoran, Okt #mecab 설치 오류\n",
    "from datasets import load_from_disk\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk('../data/train_dataset')\n",
    "train_split = train_dataset['train']\n",
    "\n",
    "questions = train_split['question'][31:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_josa_hannanum(sentence):\n",
    "    tokens = hannanum.pos(sentence)\n",
    "    filtered_tokens = [word for word, pos in tokens if 'J' not in pos]  # 조사는 'J'로 시작하는 태그\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def remove_josa_kkma(sentence):\n",
    "    tokens = kkma.pos(sentence)\n",
    "    filtered_tokens = [word for word, pos in tokens if 'J' not in pos]  # 조사는 'J'로 시작하는 태그\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def remove_josa_komoran(sentence):\n",
    "    tokens = komoran.pos(sentence)\n",
    "    filtered_tokens = [word for word, pos in tokens if 'J' not in pos]  # 조사는 'J'로 시작하는 태그\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def remove_josa_okt(sentence):\n",
    "    tokens = okt.pos(sentence)\n",
    "    filtered_tokens = [word for word, pos in tokens if pos != 'Josa']  # Okt에서 조사 제거\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1 원본: 도연주와 백승희는 무슨 사이로 알려졌나?\n",
      "  Hannanum 조사 제거 후: 도연주 백승희는 무슨 사이 알리 어 지 었나 ?\n",
      "  Kkma 조사 제거 후: 도 연주 백 승희 무슨 사이 알리 어 지 었 나 ?\n",
      "  Komoran 조사 제거 후: 도 연주 백승희 무슨 사이 알리 어 지 었 나 ?\n",
      "  Okt 조사 제거 후: 도 연주 백승희 무슨 사이 알려졌나 ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, question in enumerate(questions):\n",
    "    print(f\"질문 {i+1} 원본: {question}\")\n",
    "    \n",
    "    print(f\"  Hannanum 조사 제거 후: {remove_josa_hannanum(question)}\")\n",
    "    print(f\"  Kkma 조사 제거 후: {remove_josa_kkma(question)}\")\n",
    "    print(f\"  Komoran 조사 제거 후: {remove_josa_komoran(question)}\")\n",
    "    print(f\"  Okt 조사 제거 후: {remove_josa_okt(question)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문서 토크나이징에 걸린 시간: 0.0779초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from konlpy.tag import Okt\n",
    "import json\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# wikipedia_documents.json 파일 로드\n",
    "with open('../data/wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki_data = json.load(f)\n",
    "\n",
    "# 첫 번째 문서 텍스트 추출\n",
    "first_document = list(wiki_data.values())[0]['text']\n",
    "\n",
    "# 시간 측정 시작\n",
    "start_time = time.time()\n",
    "\n",
    "# 첫 번째 문서에 대해 Okt 토크나이징 수행\n",
    "tokenized_document = okt.morphs(first_document)\n",
    "\n",
    "# 시간 측정 종료\n",
    "end_time = time.time()\n",
    "\n",
    "# 처리 시간 계산\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"첫 번째 문서 토크나이징에 걸린 시간: {processing_time:.4f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137262e8a382409898c6fa7f204565ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8856de27929448beb4369f39c160eb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b24005f89a445a95518a4594825f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3759337863324e81aab5c98b9af96531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/591 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question 1: 대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?\n",
      "Tokenized Question 1: {'input_ids': [2, 6283, 4292, 6469, 4283, 6257, 4234, 10974, 10695, 4046, 4292, 2022, 4034, 6325, 6468, 4112, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Original Question 2: 현대적 인사조직관리의 시발점이 된 책은?\n",
      "Tokenized Question 2: {'input_ids': [2, 6533, 4199, 6596, 4084, 4497, 10425, 4234, 28316, 4007, 2412, 3432, 4112, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Original Question 3: 강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?\n",
      "Tokenized Question 3: {'input_ids': [2, 25812, 4106, 4070, 20358, 4056, 4556, 4073, 3065, 2129, 4112, 6776, 4110, 6237, 13414, 4753, 4034, 4070, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Original Question 4: 11~12세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?\n",
      "Tokenized Question 4: {'input_ids': [2, 6307, 98, 6300, 18259, 4073, 6815, 4880, 2783, 4764, 4429, 4112, 7441, 6358, 6364, 4234, 7511, 4007, 10418, 4479, 4480, 4065, 4150, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Original Question 5: 명문이 적힌 유물을 구성하는 그릇의 총 개수는?\n",
      "Tokenized Question 5: {'input_ids': [2, 12773, 4007, 15283, 12020, 4292, 6537, 4279, 4034, 10343, 4234, 3467, 17720, 4034, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# monologg/koelectra-base-v3-finetuned-korquad 토크나이저 로드\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "# Train dataset 로드\n",
    "train_dataset = load_from_disk('../data/train_dataset')\n",
    "train_split = train_dataset['train']\n",
    "\n",
    "# 질문 데이터 (앞의 5개만)\n",
    "questions = train_split['question'][:5]\n",
    "\n",
    "# 질문을 토크나이징\n",
    "tokenized_questions = [tokenizer(question) for question in questions]\n",
    "\n",
    "# 토크나이징 결과 확인\n",
    "for i, (question, tokenized) in enumerate(zip(questions, tokenized_questions)):\n",
    "    print(f\"Original Question {i+1}: {question}\")\n",
    "    print(f\"Tokenized Question {i+1}: {tokenized}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Base Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi, BM25Plus\n",
    "from datasets import load_from_disk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp_data/unq_wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki_data = json.load(f)\n",
    "\n",
    "documents = [v['text'] for v in wiki_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('./exp_data/unq_train_dataset.csv')\n",
    "\n",
    "total_queries = len(train_dataset)\n",
    "\n",
    "queries = train_dataset['question'].tolist()\n",
    "correct_doc_ids = train_dataset['doc_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Experiment Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_n_gram(tokens, n):\n",
    "    if n == 1:\n",
    "        return tokens  \n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_model(corpus, tokenizer_fn, model_type=\"okapi\", n_gram=1):\n",
    "    tokenized_corpus = [apply_n_gram(tokenizer_fn(doc), n_gram) for doc in corpus]\n",
    "\n",
    "    if model_type == \"okapi\":\n",
    "        bm25_model = BM25Okapi(tokenized_corpus)\n",
    "    elif model_type == \"plus\":\n",
    "        bm25_model = BM25Plus(tokenized_corpus)\n",
    "    else:\n",
    "        raise ValueError(\"model_type은 'okapi' 또는 'plus' 중 하나여야 합니다.\")\n",
    "    \n",
    "    return bm25_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bm25_experiment(queries, correct_doc_ids, bm25_model, tokenize_fn, topk):\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        tokenized_query = tokenize_fn(query)\n",
    "        doc_scores = bm25_model.get_scores(tokenized_query)\n",
    "        top_n_indices = doc_scores.argsort()[::-1][:topk]\n",
    "\n",
    "        correct_doc_id = correct_doc_ids[i]\n",
    "        rank = topk + 1  #\n",
    "\n",
    "        for rank_idx, doc_index in enumerate(top_n_indices):\n",
    "            if doc_index == correct_doc_id:\n",
    "                rank = rank_idx + 1\n",
    "                break\n",
    "\n",
    "        incorrect_top5 = top_n_indices[:5].tolist() if rank == topk + 1 else None\n",
    "\n",
    "        results.append({\n",
    "            'query_id': i,\n",
    "            'question': query,\n",
    "            'correct_document_id': correct_doc_id,\n",
    "            'rank': rank,\n",
    "            'incorrect_top5': incorrect_top5\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(results, total_queries, output_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    within_40 = sum(results_df['rank'] <= 40)\n",
    "    within_30 = sum(results_df['rank'] <= 30)\n",
    "    within_20 = sum(results_df['rank'] <= 20)\n",
    "    within_10 = sum(results_df['rank'] <= 10)\n",
    "    within_5 = sum(results_df['rank'] <= 5)\n",
    "    \n",
    "    within_40_ratio = within_40 / total_queries * 100\n",
    "    within_30_ratio = within_30 / total_queries * 100\n",
    "    within_20_ratio = within_20 / total_queries * 100\n",
    "    within_10_ratio = within_10 / total_queries * 100\n",
    "    within_5_ratio = within_5 / total_queries * 100\n",
    "    \n",
    "    print(f\"Experiment Results:\")\n",
    "    print(f\"topk = 40: {within_40} ({within_40_ratio:.2f}%)\")\n",
    "    print(f\"topk = 30: {within_30} ({within_30_ratio:.2f}%)\")\n",
    "    print(f\"topk = 20: {within_20} ({within_20_ratio:.2f}%)\")\n",
    "    print(f\"topk = 10: {within_10} ({within_10_ratio:.2f}%)\")\n",
    "    print(f\"topk =  5: {within_5} ({within_5_ratio:.2f}%)\")\n",
    "    \n",
    "    return within_40_ratio, within_30_ratio, within_20_ratio, within_10_ratio, within_5_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram=1):\n",
    "    \"\"\"\n",
    "    실험을 수행하는 함수\n",
    "    :param queries: 질문 리스트\n",
    "    :param correct_doc_ids: 각 질문에 대한 정답 문서 ID 리스트\n",
    "    :param documents: 검색할 문서 리스트\n",
    "    :param tokenize_fn: 쿼리와 문서를 토크나이징할 함수\n",
    "    :param model_type: 사용할 BM25 모델 타입 ('okapi' 또는 'plus')\n",
    "    :param topk: 상위 k개의 문서를 검색\n",
    "    :param total_queries: 전체 쿼리 수\n",
    "    :param output_path: 결과를 저장할 파일 경로\n",
    "    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\n",
    "    \"\"\"\n",
    "    bm25_model = get_bm25_model(documents, tokenize_fn, model_type, n_gram)\n",
    "    results = run_bm25_experiment(queries, correct_doc_ids, bm25_model, lambda query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n",
    "    analyze_experiment_results(results, total_queries, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령을', '포함한', '미국의', '행정부', '견제권을', '갖는', '국가', '기관은?']...\n",
      "질문 2: ['현대적', '인사조직관리의', '시발점이', '된', '책은?']...\n",
      "질문 3: ['강희제가', '1717년에', '쓴', '글은', '누구를', '위해', '쓰여졌는가?']...\n"
     ]
    }
   ],
   "source": [
    "def blank_tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "sample_queries = [blank_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '을', '포함', '한', '미국', '의', '행정부', '견제', '권', '을']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '의', '시발', '점', '이', '된']...\n",
      "질문 3: ['강희제', '가', '1717년', '에', '쓴', '글', '은', '누구', '를', '위해']...\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "def okt_tokenize(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "sample_queries = [okt_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제', '권', '갖다', '국가', '기관', '?']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '시발', '점', '되다', '책', '?']...\n",
      "질문 3: ['강희제', '1717년', '에', '쓸다', '글', '누구', '위해', '쓰이다', '?']...\n"
     ]
    }
   ],
   "source": [
    "def okt_tokenize_remove_josa(text):\n",
    "    tokens_pos = okt.pos(text, norm=True, stem=True)\n",
    "    tokens = [word for word, pos in tokens_pos if pos != 'Josa']\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "sample_queries = [okt_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', '포', '함', '한', '미', '국', '의']...\n",
      "질문 2: ['현', '대', '적', '인', '사', '조', '직', '관', '리', '의']...\n",
      "질문 3: ['강', '희', '제', '가', '1', '7', '1', '7', '년', '에']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text.replace(\" \", \"\"))\n",
    "\n",
    "sample_queries = [char_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', ' ', '포', '함', '한', ' ', '미']...\n",
      "질문 2: ['현', '대', '적', ' ', '인', '사', '조', '직', '관', '리']...\n",
      "질문 3: ['강', '희', '제', '가', ' ', '1', '7', '1', '7', '년']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize_space(text):\n",
    "    return list(text)\n",
    "\n",
    "sample_queries = [char_tokenize_space(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '##을', '포함', '##한', '미국', '##의', '행정부', '견제', '##권', '##을']...\n",
      "질문 2: ['현대', '##적', '인사', '##조', '##직', '##관리', '##의', '시발점', '##이', '된']...\n",
      "질문 3: ['강희', '##제', '##가', '171', '##7', '##년', '##에', '쓴', '글', '##은']...\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "monologg_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "def koelectra_tokenize(text):\n",
    "    return monologg_tokenizer.tokenize(text)\n",
    "\n",
    "sample_queries = [koelectra_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', ' ', '포', '함', '한', ' ', '미', '국']...\n",
      "질문 2: ['현', '대', '적', ' ', '인', '사', '조', '직', '관', '리']...\n",
      "질문 3: ['강', '희', '제', ' ', '1', '7', '1', '7', '년', ' ']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize_remove_josa(text):\n",
    "    tokens = list(text)\n",
    "    josa_list = ['은', '는', '이', '가', '을', '를', '에', '의', '도']\n",
    "    filtered_tokens = [token for token in tokens if token not in josa_list]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "sample_queries = [char_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### char + bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 40: 3719 (94.10%)\n",
      "topk = 30: 3680 (93.12%)\n",
      "topk = 20: 3627 (91.78%)\n",
      "topk = 10: 3496 (88.46%)\n",
      "topk =  5: 3333 (84.34%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=char_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=40, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_40_result/char_bi.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 40: 3724 (94.23%)\n",
      "topk = 30: 3686 (93.27%)\n",
      "topk = 20: 3636 (92.00%)\n",
      "topk = 10: 3499 (88.54%)\n",
      "topk =  5: 3337 (84.44%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=char_tokenize_space,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=40, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_40_result/char_space_bi.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 40: 3378 (85.48%)\n",
      "topk = 30: 3336 (84.41%)\n",
      "topk = 20: 3243 (82.06%)\n",
      "topk = 10: 3094 (78.29%)\n",
      "topk =  5: 2902 (73.43%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=koelectra_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=40, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_40_result/electra_bi.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 40: 3738 (94.59%)\n",
      "topk = 30: 3701 (93.65%)\n",
      "topk = 20: 3637 (92.03%)\n",
      "topk = 10: 3496 (88.46%)\n",
      "topk =  5: 3337 (84.44%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=char_tokenize_remove_josa,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=40, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_40_result/charjosa_bi.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 40: 3680 (93.12%)\n",
      "topk = 30: 3646 (92.26%)\n",
      "topk = 20: 3585 (90.71%)\n",
      "topk = 10: 3449 (87.27%)\n",
      "topk =  5: 3278 (82.95%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=koelectra_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=40, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_40_result/electra_uni.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

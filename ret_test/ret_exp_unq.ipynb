{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Base Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi, BM25Plus\n",
    "from bm25plus_GPU import BM25PlusGPU\n",
    "from datasets import load_from_disk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp_data/unq_wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki_data = json.load(f)\n",
    "\n",
    "documents = [v['text'] for v in wiki_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('./exp_data/unq_train_dataset.csv')\n",
    "\n",
    "total_queries = len(train_dataset)\n",
    "\n",
    "queries = train_dataset['question'].tolist()\n",
    "correct_doc_ids = train_dataset['doc_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Experiment Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_n_gram(tokens, n):\n",
    "    if n == 1:\n",
    "        return tokens  \n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_model(corpus, tokenizer_fn, model_type=\"okapi\", n_gram=1):\n",
    "    tokenized_corpus = [apply_n_gram(tokenizer_fn(doc), n_gram) for doc in corpus]\n",
    "\n",
    "    if model_type == \"okapi\":\n",
    "        bm25_model = BM25Okapi(tokenized_corpus)\n",
    "    elif model_type == \"plus\":\n",
    "        bm25_model = BM25Plus(tokenized_corpus)\n",
    "    elif model_type == \"plus_GPU\":\n",
    "        bm25_model = BM25PlusGPU(tokenized_corpus, use_gpu=use_gpu)\n",
    "    else:\n",
    "        raise ValueError(\"model_type은 'okapi' 또는 'plus' 또는 'plus_GPU'중 하나여야 합니다.\")\n",
    "    \n",
    "    return bm25_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bm25_experiment(queries, correct_doc_ids, bm25_model, tokenize_fn, topk):\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        tokenized_query = tokenize_fn(query)\n",
    "        doc_scores = bm25_model.get_scores(tokenized_query)\n",
    "        top_n_indices = doc_scores.argsort()[::-1][:topk]\n",
    "\n",
    "        correct_doc_id = correct_doc_ids[i]\n",
    "        rank = topk + 1  #\n",
    "\n",
    "        for rank_idx, doc_index in enumerate(top_n_indices):\n",
    "            if doc_index == correct_doc_id:\n",
    "                rank = rank_idx + 1\n",
    "                break\n",
    "\n",
    "        incorrect_top5 = top_n_indices[:5].tolist() if rank == topk + 1 else None\n",
    "\n",
    "        results.append({\n",
    "            'query_id': i,\n",
    "            'question': query,\n",
    "            'correct_document_id': correct_doc_id,\n",
    "            'rank': rank,\n",
    "            'incorrect_top5': incorrect_top5\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(results, total_queries, output_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    within_20 = sum(results_df['rank'] <= 20) #문서순위가 20등 이내인 것들의 개수\n",
    "    within_10 = sum(results_df['rank'] <= 10)\n",
    "    within_5 = sum(results_df['rank'] <= 5)\n",
    "    \n",
    "    within_20_ratio = within_20 / total_queries * 100\n",
    "    within_10_ratio = within_10 / total_queries * 100\n",
    "    within_5_ratio = within_5 / total_queries * 100\n",
    "    \n",
    "    print(f\"Experiment Results:\")\n",
    "    print(f\"topk = 20: {within_20} ({within_20_ratio:.2f}%)\")\n",
    "    print(f\"topk = 10: {within_10} ({within_10_ratio:.2f}%)\")\n",
    "    print(f\"topk =  5: {within_5} ({within_5_ratio:.2f}%)\")\n",
    "    \n",
    "    return within_20_ratio, within_10_ratio, within_5_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram=1):\n",
    "    \"\"\"\n",
    "    실험을 수행하는 함수\n",
    "    :param queries: 질문 리스트\n",
    "    :param correct_doc_ids: 각 질문에 대한 정답 문서 ID 리스트\n",
    "    :param documents: 검색할 문서 리스트\n",
    "    :param tokenize_fn: 쿼리와 문서를 토크나이징할 함수\n",
    "    :param model_type: 사용할 BM25 모델 타입 ('okapi' 또는 'plus')\n",
    "    :param topk: 상위 k개의 문서를 검색\n",
    "    :param total_queries: 전체 쿼리 수\n",
    "    :param output_path: 결과를 저장할 파일 경로\n",
    "    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\n",
    "    \"\"\"\n",
    "    bm25_model = get_bm25_model(documents, tokenize_fn, model_type, n_gram)\n",
    "    results = run_bm25_experiment(queries, correct_doc_ids, bm25_model, lambda query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n",
    "    analyze_experiment_results(results, total_queries, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령을', '포함한', '미국의', '행정부', '견제권을', '갖는', '국가', '기관은?']...\n",
      "질문 2: ['현대적', '인사조직관리의', '시발점이', '된', '책은?']...\n",
      "질문 3: ['강희제가', '1717년에', '쓴', '글은', '누구를', '위해', '쓰여졌는가?']...\n"
     ]
    }
   ],
   "source": [
    "def blank_tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "sample_queries = [blank_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '을', '포함', '한', '미국', '의', '행정부', '견제', '권', '을']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '의', '시발', '점', '이', '된']...\n",
      "질문 3: ['강희제', '가', '1717년', '에', '쓴', '글', '은', '누구', '를', '위해']...\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "def okt_tokenize(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "sample_queries = [okt_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제', '권', '갖다', '국가', '기관', '?']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '시발', '점', '되다', '책', '?']...\n",
      "질문 3: ['강희제', '1717년', '에', '쓸다', '글', '누구', '위해', '쓰이다', '?']...\n"
     ]
    }
   ],
   "source": [
    "def okt_tokenize_remove_josa(text):\n",
    "    tokens_pos = okt.pos(text, norm=True, stem=True)\n",
    "    tokens = [word for word, pos in tokens_pos if pos != 'Josa']\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "sample_queries = [okt_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제권', '갖는', '국가', '기관', '?']...\n",
      "질문 2: ['현대적', '인사조직관리', '시발점', '된', '책', '?']...\n",
      "질문 3: ['강희제', '1717년에', '쓴', '글', '누구', '위해', '쓰여졌는가', '?']...\n"
     ]
    }
   ],
   "source": [
    "#단어단위로 나누고 -> 조사만 제거하고 n-gram 적용해보기\n",
    "\n",
    "#바로위의 okt_tokenize_remove_josa와 비교해보세요!\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 공백으로 토큰화한 후, 조사(Josa)를 제거하는 함수\n",
    "def blank_tokenize_remove_josa(text):\n",
    "    # 공백으로 단어를 나눔\n",
    "    \n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    \n",
    "    # 각 토큰에 대해 형태소 분석을 하여 조사(Josa)를 제거\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token_pos = okt.pos(token, norm=False, stem=False)\n",
    "        # 조사(Josa)가 아닌 단어들만 필터링\n",
    "        meaningful_token = ''.join([word for word, pos in token_pos if pos != 'Josa'])\n",
    "        if meaningful_token:\n",
    "            filtered_tokens.append(meaningful_token)\n",
    "    \n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "sample_queries = [blank_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', '포', '함', '한', '미', '국', '의']...\n",
      "질문 2: ['현', '대', '적', '인', '사', '조', '직', '관', '리', '의']...\n",
      "질문 3: ['강', '희', '제', '가', '1', '7', '1', '7', '년', '에']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text.replace(\" \", \"\"))\n",
    "\n",
    "sample_queries = [char_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', ' ', '포', '함', '한', ' ', '미']...\n",
      "질문 2: ['현', '대', '적', ' ', '인', '사', '조', '직', '관', '리']...\n",
      "질문 3: ['강', '희', '제', '가', ' ', '1', '7', '1', '7', '년']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize_space(text):\n",
    "    return list(text)\n",
    "\n",
    "sample_queries = [char_tokenize_space(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '##을', '포함', '##한', '미국', '##의', '행정부', '견제', '##권', '##을']...\n",
      "질문 2: ['현대', '##적', '인사', '##조', '##직', '##관리', '##의', '시발점', '##이', '된']...\n",
      "질문 3: ['강희', '##제', '##가', '171', '##7', '##년', '##에', '쓴', '글', '##은']...\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "monologg_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "def koelectra_tokenize(text):\n",
    "    return monologg_tokenizer.tokenize(text)\n",
    "\n",
    "sample_queries = [koelectra_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Blank & Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperform_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblank_tokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./exp_result/blank_plus_n1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mperform_experiment\u001b[0;34m(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m실험을 수행하는 함수\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m:param queries: 질문 리스트\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m:param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m bm25_model \u001b[38;5;241m=\u001b[39m get_bm25_model(documents, tokenize_fn, model_type, n_gram)\n\u001b[0;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_bm25_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_n_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m analyze_experiment_results(results, total_queries, output_path)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mrun_bm25_experiment\u001b[0;34m(queries, correct_doc_ids, bm25_model, tokenize_fn, topk)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(queries):\n\u001b[1;32m      5\u001b[0m     tokenized_query \u001b[38;5;241m=\u001b[39m tokenize_fn(query)\n\u001b[0;32m----> 6\u001b[0m     doc_scores \u001b[38;5;241m=\u001b[39m \u001b[43mbm25_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     top_n_indices \u001b[38;5;241m=\u001b[39m doc_scores\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:topk]\n\u001b[1;32m      9\u001b[0m     correct_doc_id \u001b[38;5;241m=\u001b[39m correct_doc_ids[i]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/rank_bm25.py:192\u001b[0m, in \u001b[0;36mBM25Plus.get_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    190\u001b[0m doc_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_len)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 192\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(doc\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_freqs])\n\u001b[1;32m    193\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m+\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    194\u001b[0m                                        (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl) \u001b[38;5;241m+\u001b[39m q_freq))\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/rank_bm25.py:192\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m doc_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_len)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 192\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_freqs])\n\u001b[1;32m    193\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m+\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    194\u001b[0m                                        (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl) \u001b[38;5;241m+\u001b[39m q_freq))\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n1.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Blank & Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2518, 63.71%\n",
      "topk = 10: 2352, 59.51%\n",
      "topk =  5: 2155, 54.53%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"okapi\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n1.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Blank & Plus & Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 1103, 27.91%\n",
      "topk = 10: 1057, 26.75%\n",
      "topk =  5: 978, 24.75%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n2.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Char & Plus & Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3627, 91.78%\n",
      "topk = 10: 3496, 88.46%\n",
      "topk =  5: 3333, 84.34%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=char_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/char_plus_n2.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Monologg & Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3585 (90.71%)\n",
      "topk = 10: 3449 (87.27%)\n",
      "topk =  5: 3278 (82.95%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=koelectra_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/koele_plus_n.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기부터 추가한 방식들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) blank_tokenize_remove_josa & Plus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperform_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mblank_tokenize_remove_josa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplus_GPU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./exp_result/blank_tokenize_remove_josa.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mperform_experiment\u001b[0;34m(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_experiment\u001b[39m(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    실험을 수행하는 함수\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    :param queries: 질문 리스트\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     bm25_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_bm25_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     results \u001b[38;5;241m=\u001b[39m run_bm25_experiment(queries, correct_doc_ids, bm25_model, \u001b[38;5;28;01mlambda\u001b[39;00m query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n\u001b[1;32m     16\u001b[0m     analyze_experiment_results(results, total_queries, output_path)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mget_bm25_model\u001b[0;34m(corpus, tokenizer_fn, model_type, n_gram)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bm25_model\u001b[39m(corpus, tokenizer_fn, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [apply_n_gram(tokenizer_fn(doc), n_gram) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m         bm25_model \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bm25_model\u001b[39m(corpus, tokenizer_fn, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [apply_n_gram(\u001b[43mtokenizer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m, n_gram) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m         bm25_model \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mblank_tokenize_remove_josa\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m---> 19\u001b[0m     token_pos \u001b[38;5;241m=\u001b[39m \u001b[43mokt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 조사(Josa)가 아닌 단어들만 필터링\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     meaningful_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m token_pos \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJosa\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/konlpy/tag/_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[0;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[0;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn = blank_tokenize_remove_josa,  \n",
    "    model_type=\"plus_GPU\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_tokenize_remove_josa.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

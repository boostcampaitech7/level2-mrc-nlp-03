{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Base Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi, BM25Plus\n",
    "from datasets import load_from_disk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp_data/unq_wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki_data = json.load(f)\n",
    "\n",
    "documents = [v['text'] for v in wiki_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('./exp_data/unq_train_dataset.csv')\n",
    "\n",
    "total_queries = len(train_dataset)\n",
    "\n",
    "queries = train_dataset['question'].tolist()\n",
    "correct_doc_ids = train_dataset['doc_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Experiment Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_n_gram(tokens, n):\n",
    "    if n == 1:\n",
    "        return tokens  \n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_model(corpus, tokenizer_fn, model_type=\"okapi\", n_gram=1):\n",
    "    tokenized_corpus = [apply_n_gram(tokenizer_fn(doc), n_gram) for doc in corpus]\n",
    "\n",
    "    if model_type == \"okapi\":\n",
    "        bm25_model = BM25Okapi(tokenized_corpus)\n",
    "    elif model_type == \"plus\":\n",
    "        bm25_model = BM25Plus(tokenized_corpus)\n",
    "    else:\n",
    "        raise ValueError(\"model_type은 'okapi' 또는 'plus' 중 하나여야 합니다.\")\n",
    "    \n",
    "    return bm25_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bm25_experiment(queries, correct_doc_ids, bm25_model, tokenize_fn, topk):\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        tokenized_query = tokenize_fn(query)\n",
    "        doc_scores = bm25_model.get_scores(tokenized_query)\n",
    "        top_n_indices = doc_scores.argsort()[::-1][:topk]\n",
    "\n",
    "        correct_doc_id = correct_doc_ids[i]\n",
    "        rank = topk + 1  #\n",
    "\n",
    "        for rank_idx, doc_index in enumerate(top_n_indices):\n",
    "            if doc_index == correct_doc_id:\n",
    "                rank = rank_idx + 1\n",
    "                break\n",
    "\n",
    "        incorrect_top5 = top_n_indices[:5].tolist() if rank == topk + 1 else None\n",
    "\n",
    "        results.append({\n",
    "            'query_id': i,\n",
    "            'question': query,\n",
    "            'correct_document_id': correct_doc_id,\n",
    "            'rank': rank,\n",
    "            'incorrect_top5': incorrect_top5\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(results, total_queries, output_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    within_20 = sum(results_df['rank'] <= 20)\n",
    "    within_10 = sum(results_df['rank'] <= 10)\n",
    "    within_5 = sum(results_df['rank'] <= 5)\n",
    "    \n",
    "    within_20_ratio = within_20 / total_queries * 100\n",
    "    within_10_ratio = within_10 / total_queries * 100\n",
    "    within_5_ratio = within_5 / total_queries * 100\n",
    "    \n",
    "    print(f\"Experiment Results:\")\n",
    "    print(f\"topk = 20: {within_20} ({within_20_ratio:.2f}%)\")\n",
    "    print(f\"topk = 10: {within_10} ({within_10_ratio:.2f}%)\")\n",
    "    print(f\"topk =  5: {within_5} ({within_5_ratio:.2f}%)\")\n",
    "    \n",
    "    return within_20_ratio, within_10_ratio, within_5_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram=1):\n",
    "    \"\"\"\n",
    "    실험을 수행하는 함수\n",
    "    :param queries: 질문 리스트\n",
    "    :param correct_doc_ids: 각 질문에 대한 정답 문서 ID 리스트\n",
    "    :param documents: 검색할 문서 리스트\n",
    "    :param tokenize_fn: 쿼리와 문서를 토크나이징할 함수\n",
    "    :param model_type: 사용할 BM25 모델 타입 ('okapi' 또는 'plus')\n",
    "    :param topk: 상위 k개의 문서를 검색\n",
    "    :param total_queries: 전체 쿼리 수\n",
    "    :param output_path: 결과를 저장할 파일 경로\n",
    "    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\n",
    "    \"\"\"\n",
    "    bm25_model = get_bm25_model(documents, tokenize_fn, model_type, n_gram)\n",
    "    results = run_bm25_experiment(queries, correct_doc_ids, bm25_model, lambda query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n",
    "    analyze_experiment_results(results, total_queries, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령을', '포함한', '미국의', '행정부', '견제권을', '갖는', '국가', '기관은?']...\n",
      "질문 2: ['현대적', '인사조직관리의', '시발점이', '된', '책은?']...\n",
      "질문 3: ['강희제가', '1717년에', '쓴', '글은', '누구를', '위해', '쓰여졌는가?']...\n"
     ]
    }
   ],
   "source": [
    "def blank_tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "sample_queries = [blank_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '을', '포함', '한', '미국', '의', '행정부', '견제', '권', '을']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '의', '시발', '점', '이', '된']...\n",
      "질문 3: ['강희제', '가', '1717년', '에', '쓴', '글', '은', '누구', '를', '위해']...\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "def okt_tokenize(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "sample_queries = [okt_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제', '권', '갖다', '국가', '기관', '?']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '시발', '점', '되다', '책', '?']...\n",
      "질문 3: ['강희제', '1717년', '에', '쓸다', '글', '누구', '위해', '쓰이다', '?']...\n"
     ]
    }
   ],
   "source": [
    "def okt_tokenize_remove_josa(text):\n",
    "    tokens_pos = okt.pos(text, norm=True, stem=True)\n",
    "    tokens = [word for word, pos in tokens_pos if pos != 'Josa']\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "sample_queries = [okt_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', '포', '함', '한', '미', '국', '의']...\n",
      "질문 2: ['현', '대', '적', '인', '사', '조', '직', '관', '리', '의']...\n",
      "질문 3: ['강', '희', '제', '가', '1', '7', '1', '7', '년', '에']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text.replace(\" \", \"\"))\n",
    "\n",
    "sample_queries = [char_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', ' ', '포', '함', '한', ' ', '미']...\n",
      "질문 2: ['현', '대', '적', ' ', '인', '사', '조', '직', '관', '리']...\n",
      "질문 3: ['강', '희', '제', '가', ' ', '1', '7', '1', '7', '년']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize_space(text):\n",
    "    return list(text)\n",
    "\n",
    "sample_queries = [char_tokenize_space(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize_remove_josa(text):\n",
    "    tokens = list(text)\n",
    "    josa_list = ['은', '는', '이', '가', '을', '를', '에', '의', '도']\n",
    "    filtered_tokens = [token for token in tokens if token not in josa_list]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "sample_queries = [char_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '##을', '포함', '##한', '미국', '##의', '행정부', '견제', '##권', '##을']...\n",
      "질문 2: ['현대', '##적', '인사', '##조', '##직', '##관리', '##의', '시발점', '##이', '된']...\n",
      "질문 3: ['강희', '##제', '##가', '171', '##7', '##년', '##에', '쓴', '글', '##은']...\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "monologg_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "def koelectra_tokenize(text):\n",
    "    return monologg_tokenizer.tokenize(text)\n",
    "\n",
    "sample_queries = [koelectra_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '을', '포함', '하', 'ᆫ', '미국', '의', '행정부', '견제', '권']...\n",
      "질문 2: ['현대', '적', '이', 'ᆫ', '사', '조직', '관리', '의', '시발점', '이']...\n",
      "질문 3: ['강희제', '가', '1717', '년', '에', '쓰', 'ᆫ', '글', '은', '누구']...\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_tokenize(text):\n",
    "    tokens = [token.form for token in kiwi.tokenize(text)]\n",
    "    return tokens\n",
    "\n",
    "sample_queries = [kiwi_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '하', 'ᆫ', '미국', '행정부', '견제', '권', '갖', '는']...\n",
      "질문 2: ['현대', '적', '이', 'ᆫ', '사', '조직', '관리', '시발점', '되', 'ᆫ']...\n",
      "질문 3: ['강희제', '1717', '년', '쓰', 'ᆫ', '글', '누구', '위하', '어', '쓰이']...\n"
     ]
    }
   ],
   "source": [
    "def kiwi_tokenize_remove_josa(text):\n",
    "    tokens = [token.form for token in kiwi.tokenize(text) if token.tag not in ['JKS', 'JKB', 'JKO', 'JKC', 'JKG', 'JKV', 'JKQ', 'JC', 'JX']]\n",
    "    return tokens\n",
    "\n",
    "sample_queries = [kiwi_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제', '권', '갖', '국가', '기관', '?']...\n",
      "질문 2: ['현대', '사', '조직', '관리', '시발점', '책', '?']...\n",
      "질문 3: ['강희제', '1717', '쓰', '글', '누구', '쓰이', '는가', '?']...\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy.utils import Stopwords\n",
    "\n",
    "stopwords = Stopwords()\n",
    "stopwords_set = stopwords.stopwords  \n",
    "\n",
    "def kiwi_tokenize_remove_stopwords(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [token.form for token in tokens if (token.form, token.tag) not in stopwords_set]\n",
    "\n",
    "sample_queries = [kiwi_tokenize_remove_stopwords(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Blank & Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2516 (63.66%)\n",
      "topk = 10: 2356 (59.62%)\n",
      "topk =  5: 2166 (54.81%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n1.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Blank & Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2518, 63.71%\n",
      "topk = 10: 2352, 59.51%\n",
      "topk =  5: 2155, 54.53%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"okapi\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n1.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Blank & Plus & Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 1103, 27.91%\n",
      "topk = 10: 1057, 26.75%\n",
      "topk =  5: 978, 24.75%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n2.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Char & Plus & Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3627, 91.78%\n",
      "topk = 10: 3496, 88.46%\n",
      "topk =  5: 3333, 84.34%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=char_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/char_plus_n2.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Monologg & Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3585 (90.71%)\n",
      "topk = 10: 3449 (87.27%)\n",
      "topk =  5: 3278 (82.95%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=koelectra_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/koele_plus_n.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Kiwi Stopword + Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-16-le' codec can't decode bytes in position 0-1: illegal encoding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperform_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrect_doc_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkiwi_tokenize_remove_stopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./exp_result/kiwi_stopword_uni.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mperform_experiment\u001b[0;34m(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_experiment\u001b[39m(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    실험을 수행하는 함수\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    :param queries: 질문 리스트\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     bm25_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_bm25_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     results \u001b[38;5;241m=\u001b[39m run_bm25_experiment(queries, correct_doc_ids, bm25_model, \u001b[38;5;28;01mlambda\u001b[39;00m query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n\u001b[1;32m     16\u001b[0m     analyze_experiment_results(results, total_queries, output_path)\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mget_bm25_model\u001b[0;34m(corpus, tokenizer_fn, model_type, n_gram)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bm25_model\u001b[39m(corpus, tokenizer_fn, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [apply_n_gram(tokenizer_fn(doc), n_gram) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m         bm25_model \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bm25_model\u001b[39m(corpus, tokenizer_fn, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [apply_n_gram(\u001b[43mtokenizer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m, n_gram) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m         bm25_model \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mkiwi_tokenize_remove_stopwords\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkiwi_tokenize_remove_stopwords\u001b[39m(text):\n\u001b[1;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m kiwi\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mform \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m (token\u001b[38;5;241m.\u001b[39mform, token\u001b[38;5;241m.\u001b[39mtag) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords_set]\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkiwi_tokenize_remove_stopwords\u001b[39m(text):\n\u001b[1;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m kiwi\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mform \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mform\u001b[49m, token\u001b[38;5;241m.\u001b[39mtag) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords_set]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-16-le' codec can't decode bytes in position 0-1: illegal encoding"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=kiwi_tokenize_remove_stopwords,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/kiwi_stopword_uni.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Kiwi Josa + Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=kiwi_tokenize_remove_josa,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/kiwi_josa_uni.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

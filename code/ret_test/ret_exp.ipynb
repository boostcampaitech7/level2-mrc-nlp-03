{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Base Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi, BM25Plus\n",
    "from datasets import load_from_disk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp_data/unq_wikipedia_documents.json', 'r', encoding='utf-8') as f:\n",
    "    wiki_data = json.load(f)\n",
    "\n",
    "documents = [v['text'] for v in wiki_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('./exp_data/unq_train_dataset.csv')\n",
    "\n",
    "total_queries = len(train_dataset)\n",
    "\n",
    "queries = train_dataset['question'].tolist()\n",
    "correct_doc_ids = train_dataset['doc_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Experiment Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_n_gram(tokens, n):\n",
    "    if n == 1:\n",
    "        return tokens  \n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_model(corpus, tokenizer_fn=None, model_type=\"okapi\", n_gram=1):\n",
    "\n",
    "    if tokenizer_fn is not None:\n",
    "        tokenized_corpus = [apply_n_gram(tokenizer_fn(doc), n_gram) for doc in corpus]\n",
    "    else:\n",
    "        tokenized_corpus = corpus \n",
    "    \n",
    "    if model_type == \"okapi\":\n",
    "        bm25_model = BM25Okapi(tokenized_corpus)\n",
    "    elif model_type == \"plus\":\n",
    "        bm25_model = BM25Plus(tokenized_corpus)\n",
    "    else:\n",
    "        raise ValueError(\"model_type은 'okapi' 또는 'plus' 중 하나여야 합니다.\")\n",
    "    \n",
    "    return bm25_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bm25_experiment(queries, correct_doc_ids, bm25_model, tokenize_fn, topk):\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        tokenized_query = tokenize_fn(query)\n",
    "        doc_scores = bm25_model.get_scores(tokenized_query)\n",
    "        top_n_indices = doc_scores.argsort()[::-1][:topk]\n",
    "\n",
    "        correct_doc_id = correct_doc_ids[i]\n",
    "        rank = topk + 1  #\n",
    "\n",
    "        for rank_idx, doc_index in enumerate(top_n_indices):\n",
    "            if doc_index == correct_doc_id:\n",
    "                rank = rank_idx + 1\n",
    "                break\n",
    "\n",
    "        incorrect_top5 = top_n_indices[:5].tolist() if rank == topk + 1 else None\n",
    "\n",
    "        results.append({\n",
    "            'query_id': i,\n",
    "            'question': query,\n",
    "            'correct_document_id': correct_doc_id,\n",
    "            'rank': rank,\n",
    "            'incorrect_top5': incorrect_top5\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(results, total_queries, output_path):\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    within_20 = sum(results_df['rank'] <= 20)\n",
    "    within_10 = sum(results_df['rank'] <= 10)\n",
    "    within_5 = sum(results_df['rank'] <= 5)\n",
    "    \n",
    "    within_20_ratio = within_20 / total_queries * 100\n",
    "    within_10_ratio = within_10 / total_queries * 100\n",
    "    within_5_ratio = within_5 / total_queries * 100\n",
    "    \n",
    "    print(f\"Experiment Results:\")\n",
    "    print(f\"topk = 20: {within_20} ({within_20_ratio:.2f}%)\")\n",
    "    print(f\"topk = 10: {within_10} ({within_10_ratio:.2f}%)\")\n",
    "    print(f\"topk =  5: {within_5} ({within_5_ratio:.2f}%)\")\n",
    "    \n",
    "    return within_20_ratio, within_10_ratio, within_5_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, n_gram=1):\n",
    "    \"\"\"\n",
    "    실험을 수행하는 함수\n",
    "    :param queries: 질문 리스트\n",
    "    :param correct_doc_ids: 각 질문에 대한 정답 문서 ID 리스트\n",
    "    :param documents: 검색할 문서 리스트\n",
    "    :param tokenize_fn: 쿼리와 문서를 토크나이징할 함수\n",
    "    :param model_type: 사용할 BM25 모델 타입 ('okapi' 또는 'plus')\n",
    "    :param topk: 상위 k개의 문서를 검색\n",
    "    :param total_queries: 전체 쿼리 수\n",
    "    :param output_path: 결과를 저장할 파일 경로\n",
    "    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1, 즉 n-gram 없이 토크나이징)\n",
    "    \"\"\"\n",
    "    bm25_model = get_bm25_model(documents, tokenize_fn, model_type, n_gram)\n",
    "    results = run_bm25_experiment(queries, correct_doc_ids, bm25_model, lambda query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n",
    "    analyze_experiment_results(results, total_queries, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Pickle Save Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def tokenize_documents_and_save(documents, tokenize_fn, pickle_path):\n",
    "    tokenized_documents = [tokenize_fn(doc) for doc in documents]\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(tokenized_documents, f)\n",
    "    return tokenized_documents\n",
    "\n",
    "def load_tokenized_documents(pickle_path):\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment_with_documents_pickle(queries, correct_doc_ids, documents, tokenize_fn, model_type, topk, total_queries, output_path, pickle_path, n_gram=1):\n",
    "    \"\"\"\n",
    "    토큰화된 문서를 pickle로 저장하거나 불러와서 BM25 실험을 수행하는 함수\n",
    "    :param queries: 질문 리스트\n",
    "    :param correct_doc_ids: 각 질문에 대한 정답 문서 ID 리스트\n",
    "    :param documents: 검색할 문서 리스트\n",
    "    :param tokenize_fn: 쿼리와 문서를 토크나이징할 함수\n",
    "    :param model_type: 사용할 BM25 모델 타입 ('okapi' 또는 'plus')\n",
    "    :param topk: 상위 k개의 문서를 검색\n",
    "    :param total_queries: 전체 쿼리 수\n",
    "    :param output_path: 결과를 저장할 파일 경로\n",
    "    :param pickle_path: 토큰화된 문서의 pickle 파일 경로\n",
    "    :param n_gram: n-gram에서 사용할 n 값 (기본값은 1)\n",
    "    \"\"\"\n",
    "    if os.path.exists(pickle_path):\n",
    "        tokenized_documents = load_tokenized_documents(pickle_path)\n",
    "    else:\n",
    "        tokenized_documents = tokenize_documents_and_save(documents, tokenize_fn, pickle_path)\n",
    "\n",
    "    tokenized_documents_with_ngram = [apply_n_gram(doc, n_gram) for doc in tokenized_documents]\n",
    "    bm25_model = get_bm25_model(tokenized_documents_with_ngram, tokenizer_fn=None, model_type=model_type, n_gram=n_gram)\n",
    "    results = run_bm25_experiment(queries, correct_doc_ids, bm25_model, lambda query: apply_n_gram(tokenize_fn(query), n_gram), topk)\n",
    "    analyze_experiment_results(results, total_queries, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령을', '포함한', '미국의', '행정부', '견제권을', '갖는', '국가', '기관은?']...\n",
      "질문 2: ['현대적', '인사조직관리의', '시발점이', '된', '책은?']...\n",
      "질문 3: ['강희제가', '1717년에', '쓴', '글은', '누구를', '위해', '쓰여졌는가?']...\n"
     ]
    }
   ],
   "source": [
    "def blank_tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "sample_queries = [blank_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '을', '포함', '한', '미국', '의', '행정부', '견제', '권', '을']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '의', '시발', '점', '이', '된']...\n",
      "질문 3: ['강희제', '가', '1717년', '에', '쓴', '글', '은', '누구', '를', '위해']...\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "def okt_tokenize(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "sample_queries = [okt_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제', '권', '갖다', '국가', '기관', '?']...\n",
      "질문 2: ['현대', '적', '인사', '조직', '관리', '시발', '점', '되다', '책', '?']...\n",
      "질문 3: ['강희제', '1717년', '에', '쓸다', '글', '누구', '위해', '쓰이다', '?']...\n"
     ]
    }
   ],
   "source": [
    "def okt_tokenize_remove_josa(text):\n",
    "    tokens_pos = okt.pos(text, norm=True, stem=True)\n",
    "    tokens = [word for word, pos in tokens_pos if pos != 'Josa']\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "sample_queries = [okt_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', '포', '함', '한', '미', '국', '의']...\n",
      "질문 2: ['현', '대', '적', '인', '사', '조', '직', '관', '리', '의']...\n",
      "질문 3: ['강', '희', '제', '가', '1', '7', '1', '7', '년', '에']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text.replace(\" \", \"\"))\n",
    "\n",
    "sample_queries = [char_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', '을', ' ', '포', '함', '한', ' ', '미']...\n",
      "질문 2: ['현', '대', '적', ' ', '인', '사', '조', '직', '관', '리']...\n",
      "질문 3: ['강', '희', '제', '가', ' ', '1', '7', '1', '7', '년']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize_space(text):\n",
    "    return list(text)\n",
    "\n",
    "sample_queries = [char_tokenize_space(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대', '통', '령', ' ', '포', '함', '한', ' ', '미', '국']...\n",
      "질문 2: ['현', '대', '적', ' ', '인', '사', '조', '직', '관', '리']...\n",
      "질문 3: ['강', '희', '제', ' ', '1', '7', '1', '7', '년', ' ']...\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize_remove_josa(text):\n",
    "    tokens = list(text)\n",
    "    josa_list = ['은', '는', '이', '가', '을', '를', '에', '의', '도']\n",
    "    filtered_tokens = [token for token in tokens if token not in josa_list]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "sample_queries = [char_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '##을', '포함', '##한', '미국', '##의', '행정부', '견제', '##권', '##을']...\n",
      "질문 2: ['현대', '##적', '인사', '##조', '##직', '##관리', '##의', '시발점', '##이', '된']...\n",
      "질문 3: ['강희', '##제', '##가', '171', '##7', '##년', '##에', '쓴', '글', '##은']...\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "monologg_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "def koelectra_tokenize(text):\n",
    "    return monologg_tokenizer.tokenize(text)\n",
    "\n",
    "sample_queries = [koelectra_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '을', '포함', '하', 'ᆫ', '미국', '의', '행정부', '견제', '권']...\n",
      "질문 2: ['현대', '적', '이', 'ᆫ', '사', '조직', '관리', '의', '시발점', '이']...\n",
      "질문 3: ['강희제', '가', '1717', '년', '에', '쓰', 'ᆫ', '글', '은', '누구']...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_tokenize(text):\n",
    "    cleaned_text = re.sub(r'[\\U00010000-\\U0010FFFF]', '', text)\n",
    "    tokens = kiwi.tokenize(cleaned_text)\n",
    "    return [token.form for token in tokens]\n",
    "\n",
    "sample_queries = [kiwi_tokenize(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '하', 'ᆫ', '미국', '행정부', '견제', '권', '갖', '는']...\n",
      "질문 2: ['현대', '적', '이', 'ᆫ', '사', '조직', '관리', '시발점', '되', 'ᆫ']...\n",
      "질문 3: ['강희제', '1717', '년', '쓰', 'ᆫ', '글', '누구', '위하', '어', '쓰이']...\n"
     ]
    }
   ],
   "source": [
    "def kiwi_tokenize_remove_josa(text):\n",
    "    cleaned_text = re.sub(r'[\\U00010000-\\U0010FFFF]', '', text)\n",
    "    tokens = [token.form for token in kiwi.tokenize(text) if token.tag not in ['JKS', 'JKB', 'JKO', 'JKC', 'JKG', 'JKV', 'JKQ', 'JC', 'JX']]\n",
    "    return tokens\n",
    "\n",
    "sample_queries = [kiwi_tokenize_remove_josa(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 1: ['대통령', '포함', '미국', '행정부', '견제', '권', '갖', '국가', '기관', '?']...\n",
      "질문 2: ['현대', '사', '조직', '관리', '시발점', '책', '?']...\n",
      "질문 3: ['강희제', '1717', '쓰', '글', '누구', '쓰이', '는가', '?']...\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy.utils import Stopwords\n",
    "\n",
    "kiwi = Kiwi()\n",
    "stopwords = Stopwords()\n",
    "stopwords_set = stopwords.stopwords  \n",
    "\n",
    "def kiwi_tokenize_remove_stopwords(text):\n",
    "    cleaned_text = re.sub(r'[\\U00010000-\\U0010FFFF]', '', text)\n",
    "    \n",
    "    tokens = kiwi.tokenize(cleaned_text)\n",
    "    return [token.form for token in tokens if (token.form, token.tag) not in stopwords_set]\n",
    "\n",
    "sample_queries = [kiwi_tokenize_remove_stopwords(query) for query in queries[:3]]  \n",
    "for i, query in enumerate(sample_queries):\n",
    "    print(f\"질문 {i+1}: {query[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Blank & Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2516 (63.66%)\n",
      "topk = 10: 2356 (59.62%)\n",
      "topk =  5: 2166 (54.81%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n1.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Blank & Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2518, 63.71%\n",
      "topk = 10: 2352, 59.51%\n",
      "topk =  5: 2155, 54.53%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"okapi\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n1.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Blank & Plus & Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 1103, 27.91%\n",
      "topk = 10: 1057, 26.75%\n",
      "topk =  5: 978, 24.75%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=blank_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/blank_plus_n2.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Char & Plus & Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3627, 91.78%\n",
      "topk = 10: 3496, 88.46%\n",
      "topk =  5: 3333, 84.34%\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=char_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/char_plus_n2.csv',\n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Monologg & Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3585 (90.71%)\n",
      "topk = 10: 3449 (87.27%)\n",
      "topk =  5: 3278 (82.95%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=koelectra_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/koele_plus_n.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Okt + Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3567 (90.26%)\n",
      "topk = 10: 3435 (86.92%)\n",
      "topk =  5: 3254 (82.34%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=okt_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/okt_uni.csv',\n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3567 (90.26%)\n",
      "topk = 10: 3435 (86.92%)\n",
      "topk =  5: 3254 (82.34%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment_with_documents_pickle(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=okt_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/okt_uni.csv',\n",
    "    pickle_path='./pickle/okt_doc.pkl', \n",
    "    n_gram=1,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Okt + Bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2829 (71.58%)\n",
      "topk = 10: 2634 (66.65%)\n",
      "topk =  5: 2440 (61.74%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment_with_documents_pickle(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=okt_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/okt_bigram.csv',\n",
    "    pickle_path='./pickle/okt_doc.pkl', \n",
    "    n_gram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) Kiwi + Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3502 (88.61%)\n",
      "topk = 10: 3345 (84.64%)\n",
      "topk =  5: 3161 (79.98%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment_with_documents_pickle(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=kiwi_tokenize,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/kiwi_unigram.csv',\n",
    "    pickle_path='./pickle/kiwi_doc.pkl', \n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) Kiwi Stopword + Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 3500 (88.56%)\n",
      "topk = 10: 3354 (84.87%)\n",
      "topk =  5: 3176 (80.36%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment_with_documents_pickle(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=kiwi_tokenize_remove_stopwords,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/kiwi_stopword_unigram.csv',\n",
    "    pickle_path='./pickle/kiwi_sw_doc.pkl', \n",
    "    n_gram=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "topk = 20: 2434 (61.59%)\n",
      "topk = 10: 2274 (57.54%)\n",
      "topk =  5: 2080 (52.63%)\n"
     ]
    }
   ],
   "source": [
    "perform_experiment_with_documents_pickle(\n",
    "    queries=queries,\n",
    "    correct_doc_ids=correct_doc_ids, \n",
    "    documents=documents,\n",
    "    tokenize_fn=kiwi_tokenize_remove_stopwords,  \n",
    "    model_type=\"plus\",  \n",
    "    topk=20, \n",
    "    total_queries=len(queries), \n",
    "    output_path='./exp_result/kiwi_stopword_bigram.csv',\n",
    "    pickle_path='./pickle/kiwi_sw_doc.pkl', \n",
    "    n_gram=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

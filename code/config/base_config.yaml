model:
  model_name_or_path: models/roberta_original # klue/roberta-large # models/roberta_original 훈련할때 쓸 모델명이나, 이어서 훈련하거나 추론할 모델경로
  config_name: null
  tokenizer_name: null
  retrieval_tokenizer: monologg/koelectra-base-v3-finetuned-korquad # BM25로 retrieve할때만 쓰는 토크나이저 이름

data:
  train_dataset_name: ../data/train_dataset
  inference_dataset_name: ../data/test_dataset
  overwrite_cache: False
  preprocessing_num_workers: null
  max_seq_length: 384
  pad_to_max_length: False
  doc_stride: 128
  max_answer_length: 100
  eval_retrieval: True
  num_clusters: 64
  top_k_retrieval: 40
  use_faiss: False
  retrieval_type: bm25 # tfidf, bm25

train:
  batch_size: 16
  max_epoch: 2
  learning_rate: 9.0e-6
  eval_step: 1000 # batch와 epoch에 따라 step수가 결정되는데 적절히 조절. total step = 7978(train samples 토큰화되서 늘어난 개수. tokenizer에 따라 다름) * epoch / batch_size
  logging_step: 1000
  save_step: 1000
  gradient_accumulation: 1
  do_train: False
  do_eval: False
  do_predict: True
  train_output_dir: models/roberta_original
  inference_output_dir: outputs/roberta_original
  seed: 42

wandb:
  use: True
  project: odqa
  name: test # model_name_epoch_bs_learning_rate이 wandb name으로 붙는데 그 앞에 붙을 접두어
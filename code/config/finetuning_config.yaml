model:
  model_name_or_path: klue/roberta-large # klue/roberta-large # models/roberta_original 훈련할때 쓸 모델명이나, 이어서 훈련하거나 추론할 모델경로
  config_name: null
  tokenizer_name: null
  retrieval_tokenizer: monologg/koelectra-base-v3-finetuned-korquad # BM25로 retrieve할때만 쓰는 토크나이저 이름

data:
  overwrite_cache: False
  preprocessing_num_workers: null
  max_seq_length: 384
  pad_to_max_length: False
  doc_stride: 128
  max_answer_length: 100
  eval_retrieval: True
  num_clusters: 64
  top_k_retrieval: 40
  use_faiss: False
  retrieval_type: bm25 # tfidf, bm25

train:
  batch_size: 32
  max_epoch: 4
  learning_rate: 1e-5
  eval_step: 200 # 000 # batch와 epoch에 따라 step수가 결정되는데 적절히 조절. total step = 7978(train samples 토큰화되서 늘어난 개수. tokenizer에 따라 다름) * epoch / batch_size
  logging_step: 100 # 500
  save_step: 200
  gradient_accumulation: 1
  do_train: True
  do_eval: False
  do_predict: False
  train_output_dir: checkpoint/test # roberta_korquad
  seed: 42

wandb:
  use: True
  project: odqa
  name: jihunyuk # model_name_epoch_bs_learning_rate이 wandb name으로 붙는데 그 앞에 붙을 접두어